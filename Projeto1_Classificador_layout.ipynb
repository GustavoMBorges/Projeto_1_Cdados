{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Gustavo Marques Borges \n",
    "\n",
    "Nome: Luiz Eduardo Correa Santoro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aten√ß√£o:** Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re \n",
    "# nltk.download('stopwords') dowlod stopword Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Em `filename`, coloque o nome do seu arquivo de dados!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrei o arquivo Magazine_Luiza_Final.xlsx, tudo certo para prosseguir com a prova!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "filename = 'Magazine_Luiza_Final.xlsx'\n",
    "if filename in os.listdir():\n",
    "   print(f'Encontrei o arquivo {filename}, tudo certo para prosseguir com a prova!')\n",
    "else:\n",
    "   print(f'N√£o encontrei o arquivo {filename} aqui no diret√≥rio {os.getcwd()}, ser√° que voc√™ n√£o baixou o arquivo?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e n√£o relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@tavaresqq a sua troca deu certo? precisando √©...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>o pessoal da #magazineluiza  ja me conhece de ...</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@magazineluiza bom dia, cad√™ o meu retorno da ...</td>\n",
       "      <td>Muito Relevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>oi @magazineluiza o  @luansantana t√° h√° dois m...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@grondinellys @magazineluiza voc√™ s√≥ precisa d...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets      Classifica√ß√£o\n",
       "0  @tavaresqq a sua troca deu certo? precisando √©...  Muito Irrelevante\n",
       "1  o pessoal da #magazineluiza  ja me conhece de ...          Relevante\n",
       "2  @magazineluiza bom dia, cad√™ o meu retorno da ...    Muito Relevante\n",
       "3  oi @magazineluiza o  @luansantana t√° h√° dois m...  Muito Irrelevante\n",
       "4  @grondinellys @magazineluiza voc√™ s√≥ precisa d...        Irrelevante"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel(filename, coverters={'Tweets': str})\n",
    "train['Classifica√ß√£o'] = train['Classifica√ß√£o'].astype('category')\n",
    "train.Classifica√ß√£o.cat.categories = ['Muito Irrelevante','Irrelevante','Nulo','Relevante','Muito Relevante']\n",
    "train.Classifica√ß√£o.cat = pd.Categorical(train.Classifica√ß√£o, categories=['Muito Irrelevante','Irrelevante','Nulo','Relevante','Muito Relevante'], ordered=True)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@tavaresqq a sua troca deu certo? precisando √©...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>oi @magazineluiza o  @luansantana t√° h√° dois m...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>@mateus_didi @americanascom @magazineluiza @re...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>rt @heartilyshawn: ol√°, @magazineluiza o canto...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>@odeiopepino @detalhesslr @americanascom @maga...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>@sandrac58953439 oi, sandra. sabia que agora t...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>@magazineluiza oi luciana legal lugar como voc...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>@magazineluiza magalu vc √© muito gostosa</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>@elisavitorio oi. pe√ßo desculpa por isso. üòî me...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>@magazineluiza bolsonaro estava certo</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets      Classifica√ß√£o\n",
       "0    @tavaresqq a sua troca deu certo? precisando √©...  Muito Irrelevante\n",
       "3    oi @magazineluiza o  @luansantana t√° h√° dois m...  Muito Irrelevante\n",
       "6    @mateus_didi @americanascom @magazineluiza @re...  Muito Irrelevante\n",
       "12   rt @heartilyshawn: ol√°, @magazineluiza o canto...  Muito Irrelevante\n",
       "14   @odeiopepino @detalhesslr @americanascom @maga...  Muito Irrelevante\n",
       "..                                                 ...                ...\n",
       "741  @sandrac58953439 oi, sandra. sabia que agora t...  Muito Irrelevante\n",
       "743  @magazineluiza oi luciana legal lugar como voc...  Muito Irrelevante\n",
       "744           @magazineluiza magalu vc √© muito gostosa  Muito Irrelevante\n",
       "747  @elisavitorio oi. pe√ßo desculpa por isso. üòî me...  Muito Irrelevante\n",
       "749              @magazineluiza bolsonaro estava certo  Muito Irrelevante\n",
       "\n",
       "[315 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtra_Train_MI = train['Classifica√ß√£o'] == 'Muito Irrelevante'\n",
    "Train_MI = train.loc[filtra_Train_MI,:] \n",
    "\n",
    "filtra_Train_I = train['Classifica√ß√£o'] == 'Irrelevante'\n",
    "Train_I = train.loc[filtra_Train_I,:]\n",
    "\n",
    "filtra_Train_N = train['Classifica√ß√£o'] == 'Nulo'\n",
    "Train_N = train.loc[filtra_Train_N,:]\n",
    "\n",
    "filtra_Train_R = train['Classifica√ß√£o'] == 'Relevante'\n",
    "Train_R = train.loc[filtra_Train_R,:]\n",
    "\n",
    "filtra_Train_MR = train['Classifica√ß√£o'] == 'Muito Relevante'\n",
    "Train_MR = train.loc[filtra_Train_MR,:]\n",
    "\n",
    "Train = [Train_MI,Train_I,Train_N,Train_R,Train_MR]\n",
    "\n",
    "Train_MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@magazineluiza aquele que eh quero twr e nao t...</td>\n",
       "      <td>Nulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@teeeeeeiii @magazineluiza kkkkk o cora√ß√£o qua...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@onofrifernando oi! te chamei no dm da uma olh...</td>\n",
       "      <td>Muito Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@felliperadama @americanascom @magazineluiza @...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@robrtplant @magazineluiza bom dia! informamos...</td>\n",
       "      <td>Nulo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets      Classifica√ß√£o\n",
       "0  @magazineluiza aquele que eh quero twr e nao t...               Nulo\n",
       "1  @teeeeeeiii @magazineluiza kkkkk o cora√ß√£o qua...        Irrelevante\n",
       "2  @onofrifernando oi! te chamei no dm da uma olh...  Muito Irrelevante\n",
       "3  @felliperadama @americanascom @magazineluiza @...        Irrelevante\n",
       "4  @robrtplant @magazineluiza bom dia! informamos...               Nulo"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\n",
    "test['Classifica√ß√£o'] = test['Classifica√ß√£o'].astype('category')\n",
    "test.Classifica√ß√£o.cat.categories = ['Muito Irrelevante','Irrelevante','Nulo','Relevante','Muito Relevante']\n",
    "test.Classifica√ß√£o.cat = pd.Categorical(test.Classifica√ß√£o, categories=['Muito Irrelevante','Irrelevante','Nulo','Relevante','Muito Relevante'], ordered=True)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtra_Test_MI = test['Classifica√ß√£o'] == 'Muito Irrelevante'\n",
    "Test_MI = test.loc[filtra_Test_MI,:] \n",
    "filtra_Test_I = train['Classifica√ß√£o'] == 'Irrelevante'\n",
    "Test_I = test.loc[filtra_Test_I,:]\n",
    "filtra_Test_N = train['Classifica√ß√£o'] == 'Nulo'\n",
    "Test_N = test.loc[filtra_Test_N,:]\n",
    "filtra_Test_R = train['Classifica√ß√£o'] == 'Relevante'\n",
    "Test_R = train.loc[filtra_Test_R,:]\n",
    "filtra_Test_MR = train['Classifica√ß√£o'] == 'Muito Relevante'\n",
    "Test_MR = test.loc[filtra_Test_MR,:]\n",
    "Test = [Test_I,Test_MI,Test_N,Test_R,Test_MR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso produto √© a **Magazine Luiza**, uma empresa varejista de eletr√¥nicos e im√≥veis. Possui lojas f√≠sicas, mas tem maior notoriedade em sua plataforma digital, cujas cr√≠ticas e elogios est√£o fortemente presente na conta do Tweeter da empresa.\n",
    "\n",
    "Para a classifica√ß√£o dos tweets envolvendo a Magazine Luiza, consideramos:\n",
    "   * **‚ÄúMuito relevantes‚Äù** aqueles que possuem problemas ou elogios √† marca, detalhando o motivo de sua cr√≠tica \n",
    "   * **‚ÄúRelevantes‚Äù** os menos detalhados.\n",
    "   * **‚ÄúNulo‚Äù** s√£o aqueles que demandam cupons de desconto para a marca e tweets com respostas √† pr√≥pria marca, que aparecem, na maioria das vezes, sem contexto. \n",
    "   * **‚ÄúIrrelevantes‚Äù** foram os coment√°rios ofensivos, que mostravam indigna√ß√£o com a marca, mas utilizando apenas palavras de baixo cal√£o.\n",
    "   * **‚ÄúMuito irrelevante‚Äù** foi marcado para os tweets que a pr√≥pria marca fazia, respondendo os clientes ou para os tweets que n√£o tinham nada a ver com a marca tampouco com o que ela prop√µe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a fun√ß√£o para tratar os dados \n",
    "def cleanup(tweet):\n",
    "    #cria uma lista para a lista de palavaras filtradas\n",
    "    tweet_limpo = []\n",
    "    #defini√ß√£o dos m√©todos de tratamento do dataframe\n",
    "    punctuation = '[@!-.:?;]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese') \n",
    "    text_subbed = re.sub(pattern, '', tweet)\n",
    "    #Limpa stopwords\n",
    "    token_words = TweetTokenizer().tokenize(text_subbed)\n",
    "    for word in token_words:\n",
    "        if word not in stopwords:\n",
    "            tweet_limpo.append(word)\n",
    "    return tweet_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/DuduSantoro/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/DuduSantoro/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8d35b8a64b08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlista_tweets_limpos\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlista_de_tweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrain_TwL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mlista_tweets_limpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_TwL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mTrain_Limpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_tweets_limpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-db479ed779aa>\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[@!-.:?;]'\u001b[0m \u001b[0;31m# Note que os sinais [] s√£o delimitadores de um conjunto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'portuguese'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtext_subbed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#Limpa stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/DuduSantoro/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Limpa os tweets de acordo com  a sua relev√¢ncia \n",
    "Train_Limpo = []\n",
    "for dataframe in Train:\n",
    "    lista_de_tweets = dataframe['Tweets'].values.tolist() \n",
    "    lista_tweets_limpos =[]\n",
    "    for tweet in lista_de_tweets:\n",
    "        train_TwL = cleanup(tweet)\n",
    "        lista_tweets_limpos.extend(train_TwL)\n",
    "    Train_Limpo.append(lista_tweets_limpos)\n",
    "    \n",
    "#Calcula e aloca em vari√°vel a probabilidade de ser relevante\n",
    "prob_Re = train['Classifica√ß√£o'].value_counts(True, sort=False)\n",
    "P_MI = prob_Re[0]\n",
    "P_I = prob_Re[1]\n",
    "P_N = prob_Re[2]\n",
    "P_R = prob_Re[3]\n",
    "P_MR = prob_Re[4]\n",
    "\n",
    "P_Re = [P_MI,P_I,P_N,P_R,P_MR]\n",
    "\n",
    "#Separa as listas provevineites dos dataframes\n",
    "palavras_train_MI = pd.Series(Train_Limpo[0]).value_counts().to_dict()\n",
    "palavras_train_I = pd.Series(Train_Limpo[1]).value_counts().to_dict()\n",
    "palavras_train_N = pd.Series(Train_Limpo[2]).value_counts().to_dict()\n",
    "palavras_train_R = pd.Series(Train_Limpo[3]).value_counts().to_dict()\n",
    "palavras_train_MR = pd.Series(Train_Limpo[4]).value_counts().to_dict()\n",
    "\n",
    "dic_Fa_palavras=[palavras_train_MI, palavras_train_I,palavras_train_N,palavras_train_R,palavras_train_MR] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_Bayes(tweet):\n",
    "    clean_tweet = cleanup(tweet)\n",
    "    serie_de_fa =[]\n",
    "    for dic in dic_Fa_palavras:\n",
    "        dic_f ={}\n",
    "        for palavra in clean_tweet:\n",
    "            if palavra in dic:\n",
    "                dic_f[palavra] = dic[palavra]\n",
    "            if palavra not in dic:\n",
    "                dic_f[palavra] = 0\n",
    "        serie_de_fa.append(pd.Series(dic_f))\n",
    "        \n",
    "    fa_palavras_MI = serie_de_fa[0]\n",
    "    fa_palavras_I = serie_de_fa[1]\n",
    "    fa_palavras_N = serie_de_fa[2]\n",
    "    fa_palavras_R = serie_de_fa[3]\n",
    "    fa_palavras_MR = serie_de_fa[4]\n",
    "    fa_palavras_Re = [fa_palavras_MI,fa_palavras_I,fa_palavras_N,fa_palavras_R,fa_palavras_MR]  \n",
    "    lista_prob = []\n",
    "    alpha = 0.01\n",
    "    for i in range(0,len(fa_palavras_Re)):\n",
    "        contagem_palavra_mais_alpha = fa_palavras_Re[i].add(alpha)\n",
    "        contagem_todas_palavras = sum(dic_Fa_palavras[i].values())\n",
    "        laplace_smothing = contagem_palavra_mais_alpha.divide(contagem_todas_palavras + alpha*(len(dic_Fa_palavras[i])))\n",
    "        smoothing = np.log10(laplace_smothing)\n",
    "        P_palavras_dado_Re = smoothing.sum()\n",
    "        lista_prob.append(P_palavras_dado_Re + (np.log10(P_Re[i])))\n",
    "    MAX = 0\n",
    "    for k in range(0,len(lista_prob)):\n",
    "        if lista_prob[k] >= lista_prob[MAX]:\n",
    "            MAX = k\n",
    "    if MAX == 0:\n",
    "        Relevancia_NV = 'Muito Irrelevante'\n",
    "    if MAX == 1:\n",
    "        Relevancia_NV = 'Irrelevante'\n",
    "    if MAX == 2:\n",
    "        Relevancia_NV = 'Nulo'\n",
    "    if MAX == 3:\n",
    "        Relevancia_NV = 'Relevante'\n",
    "    if MAX == 4:\n",
    "        Relevancia_NV = 'Muito Relevante'\n",
    "\n",
    "    return Relevancia_NV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_NaiveBayes_train = []\n",
    "for i in train['Tweets']:\n",
    "    Cat_NaiveBayes_train.append(Naive_Bayes(i))\n",
    "train['Relevancia_NB'] = Cat_NaiveBayes_train\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_NaiveBayes_test = []\n",
    "for i in test['Tweets']:\n",
    "    Cat_NaiveBayes_test.append(Naive_Bayes(i))\n",
    "test['Relevancia_NB'] = Cat_NaiveBayes_test\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancia = ['Muito Irrelevante','Irrelevante','Nulo','Relevante','Muito Relevante']\n",
    "Relevancia_NB_X_Classificacao_train = pd.crosstab(train.Classifica√ß√£o,train.Relevancia_NB,normalize=True)[relevancia]\n",
    "Relevancia_NB_X_Classificacao_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A propor√ß√£o de acerto do nosso classificador √© {(Relevancia_NB_X_Classificacao_train.iloc[0,0]+Relevancia_NB_X_Classificacao_train.iloc[1,1]+Relevancia_NB_X_Classificacao_train.iloc[2,2]+Relevancia_NB_X_Classificacao_train.iloc[3,3]+Relevancia_NB_X_Classificacao_train.iloc[4,4])*100:.5g}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Relevancia_NB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ec3d54ed8a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRelevancia_NB_X_Classificacao_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassifica√ß√£o\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRelevancia_NB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelevancia\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mRelevancia_NB_X_Classificacao_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Relevancia_NB'"
     ]
    }
   ],
   "source": [
    "Relevancia_NB_X_Classificacao_test = pd.crosstab(test.Classifica√ß√£o,test.Relevancia_NB,normalize= True)[relevancia]\n",
    "Relevancia_NB_X_Classificacao_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Efetividade_por_categoria_test = pd.crosstab(test.Classifica√ß√£o,test.Relevancia_NB,normalize='index')[relevancia]\n",
    "Efetividade_por_categoria_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Relevancia_NB_X_Classificacao_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3ff2d13eefe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'A propor√ß√£o de acerto do nossvo classificador √© {(Relevancia_NB_X_Classificacao_test.iloc[0,0]+Relevancia_NB_X_Classificacao_test.iloc[1,1]+Relevancia_NB_X_Classificacao_test.iloc[2,2]+Relevancia_NB_X_Classificacao_test.iloc[3,3]+Relevancia_NB_X_Classificacao_test.iloc[4,4])*100:.5g}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'A propor√ß√£o de acerto da categoria Muito Irrelevante √© {Efetividade_por_categoria_test.iloc[0,0]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[0,1]+Efetividade_por_categoria_test.iloc[0,2]+Efetividade_por_categoria_test.iloc[0,3]+Efetividade_por_categoria_test.iloc[0,4])*100:.5g}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'A propor√ß√£o de acerto da categoria Irrelevante √© {Efetividade_por_categoria_test.iloc[1,1]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[1,0]+Efetividade_por_categoria_test.iloc[1,2]+Efetividade_por_categoria_test.iloc[1,3]+Efetividade_por_categoria_test.iloc[1,4])*100:.5g}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'A propor√ß√£o de acerto da categoria Nulo √© {Efetividade_por_categoria_test.iloc[2,2]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[2,1]+Efetividade_por_categoria_test.iloc[2,0]+Efetividade_por_categoria_test.iloc[2,3]+Efetividade_por_categoria_test.iloc[2,4])*100:.5g}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'A propor√ß√£o de acerto da categoria Relevante √© {Efetividade_por_categoria_test.iloc[3,3]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[3,1]+Efetividade_por_categoria_test.iloc[3,2]+Efetividade_por_categoria_test.iloc[3,0]+Efetividade_por_categoria_test.iloc[3,4])*100:.5g}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Relevancia_NB_X_Classificacao_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'A propor√ß√£o de acerto do nossvo classificador √© {(Relevancia_NB_X_Classificacao_test.iloc[0,0]+Relevancia_NB_X_Classificacao_test.iloc[1,1]+Relevancia_NB_X_Classificacao_test.iloc[2,2]+Relevancia_NB_X_Classificacao_test.iloc[3,3]+Relevancia_NB_X_Classificacao_test.iloc[4,4])*100:.5g}%')\n",
    "print(f'A propor√ß√£o de acerto da categoria Muito Irrelevante √© {Efetividade_por_categoria_test.iloc[0,0]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[0,1]+Efetividade_por_categoria_test.iloc[0,2]+Efetividade_por_categoria_test.iloc[0,3]+Efetividade_por_categoria_test.iloc[0,4])*100:.5g}%') \n",
    "print(f'A propor√ß√£o de acerto da categoria Irrelevante √© {Efetividade_por_categoria_test.iloc[1,1]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[1,0]+Efetividade_por_categoria_test.iloc[1,2]+Efetividade_por_categoria_test.iloc[1,3]+Efetividade_por_categoria_test.iloc[1,4])*100:.5g}%')\n",
    "print(f'A propor√ß√£o de acerto da categoria Nulo √© {Efetividade_por_categoria_test.iloc[2,2]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[2,1]+Efetividade_por_categoria_test.iloc[2,0]+Efetividade_por_categoria_test.iloc[2,3]+Efetividade_por_categoria_test.iloc[2,4])*100:.5g}%')\n",
    "print(f'A propor√ß√£o de acerto da categoria Relevante √© {Efetividade_por_categoria_test.iloc[3,3]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[3,1]+Efetividade_por_categoria_test.iloc[3,2]+Efetividade_por_categoria_test.iloc[3,0]+Efetividade_por_categoria_test.iloc[3,4])*100:.5g}%')\n",
    "print(f'A propor√ß√£o de acerto da categoria Muito Relevante √© {Efetividade_por_categoria_test.iloc[4,4]*100:.5g}% e de erros eh {(Efetividade_por_categoria_test.iloc[4,1]+Efetividade_por_categoria_test.iloc[4,2]+Efetividade_por_categoria_test.iloc[4,3]+Efetividade_por_categoria_test.iloc[4,0])*100:.5g}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostra_train = pd.DataFrame()\n",
    "amostra_train['Treinamento Naive Bayes'] = train.loc[:,'Relevancia_NB'].value_counts(normalize = 'index')\n",
    "amostra_train['Treinamento'] = train.loc[:,'Classifica√ß√£o'].value_counts(normalize = 'index')\n",
    "amostra_test = pd.DataFrame()\n",
    "amostra_test['Teste Naive Bayes'] = test.loc[:,'Relevancia_NB'].value_counts(normalize = 'index')\n",
    "amostra_test['Teste'] = test.loc[:,'Classifica√ß√£o'].value_counts(normalize = 'index')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax1 =fig.add_subplot(221)\n",
    "ax2 =fig.add_subplot(222)\n",
    "\n",
    "analise_train = amostra_train.plot.bar(ax=ax1)\n",
    "analise_test = amostra_test.plot.bar(ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "### Plano de Extens√£o\n",
    "\n",
    "Como plano de expans√£o para a melhoria do nosso classificador, pretendemos diferenciar os empecilhos do clientes, pois ao longo do projeto, percebemos que as cr√≠ticas s√£o divididas por: aqueles insatisfeitos com o servi√ßo de entrega dos produtos da Magazine Luiza, e aqueles insatisfeitos com o servi√ßo de atendimento online, ou seja, com a pr√≥pria conta de Tweeter da marca. Basicamente, dentro das categorias \"Muito relevante\" e \"Relevante\", criaremos duas subcategorias, que ser√£o os dois tipos de cr√≠ticas em rela√ß√£o √† marca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfei√ßoamento:\n",
    "\n",
    "Os trabalhos v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separa√ß√£o de espa√ßos entre palavras e emojis ou entre emojis e emojis\n",
    "* Propor outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o ou classifica√ß√£o\n",
    "* Criar categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que n√£o posso usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que realiza an√°lise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
